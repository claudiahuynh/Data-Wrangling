p8105_mh4589_hw2
================
My An Huynh
2024-09-25

Import the `subway` dataset

``` r
subway_df = 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA", "", ".")) |>
  janitor::clean_names() |> 
  mutate( 
   entry = ifelse(is.na(entry), FALSE, TRUE)) |> 
  select(line, station_name, station_latitude, station_longitude, route1:route11, entry, vending, entrance_type, ada) 
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

This dataset contains information on the NYC subway system. It includes
division, line, station names within each line, and the routes each line
serves. It also includes information on vending, staffing and
accessibility (whether the station is ADA compliant, and entrance
types). It also includes geographical information such as north-south
streets, east-west streets, corner(NW, SW, etc), station location,
longitude and latitude as well as entry location, longitude and
latitude.

I retained these following variables: line, station name, station
latitude, station longitude, routes served, entry, vending, entrance
type and ADA compliance. The resulting data set contains 19 columns and
1868 rows.

The data is still not very tidy.

There are 465 distinct stations by line and by name. There are a total
of 468 stations that are ADA compliant. There are 183 stations out of
1868 stations that allow entry without vending.

\#Problem 2 Import `mr_trash` dataset.

``` r
mr_trash_df = 
  read_excel(
     "data/202309 Trash Wheel Collection Data.xlsx", 
    sheet = "Mr. Trash Wheel",
    range = "A2:N586",
    na = c("NA", "", ".")) |> 
  janitor::clean_names() |> 
  mutate(
    sports_balls = as.integer(sports_balls),
    year = as.numeric(year),
    trash_wheel = "mr_trash"
  ) |> 
  relocate(dumpster, trash_wheel)
```

Import `prof_trash` dataset.

``` r
prof_trash_df = 
  read_excel(
     "data/202309 Trash Wheel Collection Data.xlsx", 
    sheet = "Professor Trash Wheel",
    na = c("NA", "", "."),
    range = "A2:M108"
  ) |> 
  janitor::clean_names()  |> 
  mutate(
    trash_wheel = "prof_trash"
  ) 
```

Import `gwynnda` dataset.

``` r
gwynnda_df = 
  read_excel(
     "data/202309 Trash Wheel Collection Data.xlsx", 
    sheet = "Gwynnda Trash Wheel",
    na = c("NA", "", "."),
    range = "A2:L157"
  ) |> 
  janitor::clean_names() |> 
  mutate(
    trash_wheel = "gwynnda"
  )
```

Combine 3 datasets.

``` r
trash_wheel_df = 
  bind_rows(mr_trash_df, prof_trash_df, gwynnda_df) |> 
  janitor::clean_names() |> 
  relocate(dumpster, trash_wheel) |> 
  select(dumpster:homes_powered)
```

I combined the 3 datasets (Mr Trash Wheel, Professor Trash Wheel and
Gwynnda) into one dataset named trash_wheel_df. There are a total of 845
rows and 15 columns. The combined dataset `trash_wheel_df` provides
information on the weight in tons and volume in cubic yards of trash
each trash wheel removes from the Inner Harbor in Baltimore from May
2016 to June 2023. The dataset also provides information on the
different types of trash (e.g plastic bottles, cigarette buds, sports
balls, etc) and the quantity for each type of trash that each trash
wheel removes from May 2016 to June 2023. The `homes_powered` variable
shows the number of houses powered for every ton of trash removed.

The total weight of trash collected by Professor Trash Wheel is 216.26
tons. The total number of cigarette butts collected by Gwynnda in June
of 2022 is 1.812^{4} butts.

## Problem 3

Import `bakers` dataset.

``` r
bakers_df = 
  read_csv("data/bakers.csv", na = c("NA", "", ".")) |> 
  janitor::clean_names() |> 
  na.omit(bakers_df) |> 
  separate(
    baker_name, into = c("baker", "baker_last_name"), sep = " "
  ) |> 
  arrange(series)
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Import `bakes` dataset.

``` r
bakes_df = 
  read_csv("data/bakes.csv", na = c("NA", "", ".")) |> 
  janitor::clean_names() |> 
  mutate(
    baker = str_replace_all(baker, '"Jo"', "Jo")
  )
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Import `results` dataset.

``` r
results_df = 
  read_csv("data/results.csv", na = c("NA", "", "."), skip = 2) |> 
  janitor::clean_names() |> 
  arrange(series, baker) |> 
  na.omit(results_df) |> 
  mutate(
    baker = ifelse(row_number() == 57:64, "Jo", baker)
  )
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Use anti_join to view discrepancies.

``` r
missing_bakes_df = anti_join(bakers_df, bakes_df)
```

    ## Joining with `by = join_by(baker, series)`

``` r
missing_results_df = anti_join(bakers_df, results_df)
```

    ## Joining with `by = join_by(baker, series)`

Join the three datasets.

``` r
results_and_bakes_df = 
  left_join(bakes_df, results_df, by = c("baker", "series", "episode"))

joined_gbb_df =
  left_join(results_and_bakes_df, bakers_df, by = c("baker", "series"))
```

I imported the three datasets: `bakers`, `bakes` and `results` and
cleaned variable names. For the `bakers` dataset, I separated the
baker’s full names to two columns - first and last name. For the `bakes`
dataset, I used str_replace_all to get rid of the quotation marks around
“Jo”. I replaced Joanne with Jo in the `results` dataset for
consistency. I noticed these discrepancies by using `anti_join()`. The
results from using `anti_join()` also show that the `bakes` dataset only
show information from series 1 to 8.

I used `left_join` to merge all 3 datasets into a single dataset named
`joined_gbb_df`. The dataset includes information on the bakers’
background (age, hometown, etc), signature bakes, show stoppers, and
their results (whether they move forward or get eliminated). Because the
`bakes` dataset is incomplete, the joined dataset also only includes
information up to series 8.

``` r
winners_df =
  results_df |> 
  filter(series <= 10, series >=5) |> 
  filter(result %in% c("WINNER", "STAR BAKER"))  |> 
  select(series, episode, baker, result)

winners_df |>
  pivot_wider(
    names_from = series,
    values_from = baker
  ) |> 
  arrange(episode) |> 
  knitr::kable()
```

| episode | result     | 5       | 6      | 7         | 8      | 9       | 10       |
|--------:|:-----------|:--------|:-------|:----------|:-------|:--------|:---------|
|       1 | STAR BAKER | Nancy   | Marie  | Jane      | Steven | Manon   | Michelle |
|       2 | STAR BAKER | Richard | Ian    | Candice   | Steven | Rahul   | Alice    |
|       3 | STAR BAKER | Luis    | Ian    | Tom       | Julia  | Rahul   | Michael  |
|       4 | STAR BAKER | Richard | Ian    | Benjamina | Kate   | Dan     | Steph    |
|       5 | STAR BAKER | Kate    | Nadiya | Candice   | Sophie | Kim-Joy | Steph    |
|       6 | STAR BAKER | Chetna  | Mat    | Tom       | Liam   | Briony  | Steph    |
|       7 | STAR BAKER | Richard | Tamal  | Andrew    | Steven | Kim-Joy | Henry    |
|       8 | STAR BAKER | Richard | Nadiya | Candice   | Stacey | Ruby    | Steph    |
|       9 | STAR BAKER | Richard | Nadiya | Andrew    | Sophie | Ruby    | Alice    |
|      10 | WINNER     | Nancy   | Nadiya | Candice   | Sophie | Rahul   | David    |

I created a reader-friendly table that extracts information from the
`results` dataset and includes information on the star bakers and
winners across seasons 5 to 10. I noticed some predictability in that
the winners in series 5 to 9 won “star baker” at least once. The only
exception is the winner in series 10, David, who had never won star
baker.

Import the `views` dataset

``` r
viewership_df = 
  read_csv("data/viewers.csv") |> 
  janitor::clean_names() |> 
  na.omit(views_df) |> 
  pivot_longer(
    cols = series_1:series_10,
    names_to = "series",
    values_to = "viewership",
    names_prefix = "series_"
  ) |> 
  mutate(
    series = as.numeric(series)
  ) |> 
  arrange(series) |> 
  relocate(series)
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
knitr::kable(head(viewership_df, 10), caption = "First 10 rows in the Viewership dataset")
```

| series | episode | viewership |
|-------:|--------:|-----------:|
|      1 |       1 |       2.24 |
|      1 |       2 |       3.00 |
|      1 |       3 |       3.00 |
|      1 |       4 |       2.60 |
|      1 |       5 |       3.03 |
|      1 |       6 |       2.75 |
|      2 |       1 |       3.10 |
|      2 |       2 |       3.53 |
|      2 |       3 |       3.82 |
|      2 |       4 |       3.60 |

First 10 rows in the Viewership dataset

The average viewership in series 1 is 2.77. The average viewership in
series 5 is 9.485.
